What is spark architecture and its components?

PySpark architecture follows Apache Spark's master-worker model for distributed data processing, with Python API integration via Py4J. It scales from single machine to thousands of nodes through these core components

User Code → Driver → DAG Scheduler → Task Scheduler → Executors

Core components:
* Driver programs
* Executors
* CLuster management

Difference between hadoop and spark:

| Aspect       | Hadoop                  | Spark                                  |
| ------------ | ----------------------- | -------------------------------------- |
| Processing   | Disk-based MapReduce    | In-memory RDDs/DataFrames chaosgenius​ |
| Speed        | 10x slower than Spark   | 100x faster for in-memory ops          |
| Architecture | HDFS + YARN + MapReduce | Driver + Executors + Cluster Manager   |
| Real-time    | Batch only              | Streaming + Interactive                |
| Storage      | HDFS (distributed FS)   | Memory first, disk spillover           |
| Use Cases    | ETL, batch analytics    | ML, streaming, real-time analytics     |

What is spark context and spark session

SparkContext is the original low-level entry point for Spark, handling RDD operations and cluster connection. SparkSession (Spark 2.0+) is the modern unified entry point that wraps SparkContext and adds DataFrame/SQL/Streaming support.

| Aspect    | SparkContext                      | SparkSession                                      |
| --------- | --------------------------------- | ------------------------------------------------- |
| Purpose   | RDD operations only               | RDD + DataFrame + SQL + Streaming                 |
| Creation  | SparkContext("appName", "master") | SparkSession.builder.appName("app").getOrCreate() |
| Internals | Standalone                        | Contains SparkContext + SQLContext + HiveContext  |
| Usage     | Legacy code                       | Modern PySpark standard                           |
| Access    | Direct                            | spark.sparkContext for RDDs                       |

Map vs flatMap
map** transforms each element to one output (1:1), while flatMap transforms each element to multiple outputs and flattens (1:many → 1 stream)

Difference between GroupByKey vs ReducedByKey

| Aspect        | groupByKey                | reduceByKey                          |
| ------------- | ------------------------- | ------------------------------------ |
| Data Movement | All values shuffled first | Local combine → shuffle reduced data |
| Output        | (key, Iterable[values])   | (key, single_reduced_value)          |
| Performance   | ❌ Expensive shuffle       | ✅ 10x faster for large data          |
| Memory        | Loads all values per key  | Processes incrementally              |

Difference between RDD, DataFrame, Dataset

| Feature      | RDD                      | DataFrame                      | Dataset                         |
| ------------ | ------------------------ | ------------------------------ | ------------------------------- |
| Introduced   | Spark 1.0                | Spark 1.3                      | Spark 1.6 (Scala), PySpark 2.0+ |
| Structure    | Unstructured             | Named columns (like SQL table) | Typed objects + columns         |
| Optimization | Manual                   | Catalyst + Tungsten ✓          | Catalyst + Tungsten ✓           |
| Type Safety  | Compile-time ✓           | Runtime only                   | Compile-time ✓ (Scala)          |
| API Style    | Functional (map, filter) | SQL + functional               | Both                            |

Job, Stage, Executor, Cluster manager, tasks

| Component       | Role                                           | Your Home Credit Context             |
| --------------- | ---------------------------------------------- | ------------------------------------ |
| Job             | Full transformation + action (e.g. df.count()) | Complete ETL pipeline                |
| Stage           | Sequence of transformations w/o shuffle        | filter() → map() = 1 stage           |
| Task            | Stage execution on 1 partition                 | Process 1/100 Home Credit partitions |
| Executor        | JVM process on worker node (4GB, 2 cores)      | Runs 5 tasks in parallel             |
| Cluster Manager | Allocates Executors (YARN/Standalone/K8s)      | Manages 10 worker nodes              |
